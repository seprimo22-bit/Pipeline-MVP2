from fastapi import FastAPI, Request
from pydantic import BaseModel
from fastapi.responses import HTMLResponse
from fastapi.templating import Jinja2Templates
import re
import os
from openai import OpenAI

app = FastAPI(title="Campbell Cognitive Pipeline")

# Template folder (make sure templates/index.html exists)
templates = Jinja2Templates(directory="templates")

# OpenAI client (uses your Render environment variable)
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))


# ---------- INPUT MODEL ----------
class QuestionInput(BaseModel):
    question: str


# ---------- AI FACT RETRIEVAL ----------
def get_ai_answer(question):
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": "Provide factual, neutral, concise information."
                },
                {
                    "role": "user",
                    "content": question
                }
            ],
            temperature=0.3
        )

        return response.choices[0].message.content

    except Exception as e:
        return f"AI retrieval error: {str(e)}"


# ---------- PAPER ZERO ----------
def paper_zero_layer(text):
    sentences = re.split(r'[.!?]', text)

    facts, assumptions, unknowns = [], [], []

    for s in sentences:
        s = s.strip()
        if not s:
            continue

        if "maybe" in s.lower() or "probably" in s.lower():
            assumptions.append(s)
        else:
            facts.append(s)

    return {
        "facts": facts,
        "assumptions": assumptions,
        "unknowns": unknowns
    }


# ---------- ORR CORE ----------
def orr_core(data):
    cleaned = list(set(data["facts"]))
    return {
        "observations": cleaned,
        "contradictions_removed": True,
        "bias_checked": True
    }


# ---------- ORNS ----------
def orns_stabilization(data):
    return {
        "stable_interpretation": data["observations"],
        "ambiguity_checked": True,
        "emotional_bias_reduced": True
    }


# ---------- AXIOM EXTRACTION ----------
def axiom_extraction(data):
    axioms = []
    for item in data["stable_interpretation"]:
        if len(item.split()) > 4:
            axioms.append(f"Potential principle: {item}")
    return axioms


# ---------- DECISION FRAMEWORK ----------
def extended_decision_framework(data):
    return {
        "risk_level": "unknown",
        "ethical_flag": "neutral",
        "structural_integrity": "stable"
    }


# ---------- FINAL PASS ----------
def final_orr_pass(data):
    return {
        "verified_output": data["stable_interpretation"],
        "narrative_creep_removed": True
    }


# ---------- OUTPUT CLASSIFICATION ----------
def output_classification(data):
    return {
        "facts": data["verified_output"],
        "hypotheses": [],
        "speculation": [],
        "questions": []
    }


# ---------- FULL PIPELINE ----------
def run_pipeline(question):
    ai_answer = get_ai_answer(question)

    pz = paper_zero_layer(ai_answer)
    orr = orr_core(pz)
    orns = orns_stabilization(orr)
    axioms = axiom_extraction(orns)
    extended = extended_decision_framework(orns)
    final = final_orr_pass(orns)
    classified = output_classification(final)

    return {
        "original_question": question,
        "ai_answer": ai_answer,
        "paper_zero": pz,
        "orr": orr,
        "orns": orns,
        "axioms": axioms,
        "decision_framework": extended,
        "final_output": classified
    }


# ---------- ROUTES ----------

# THIS FIXES YOUR UI SERVING ISSUE
@app.get("/", response_class=HTMLResponse)
def home(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})


@app.post("/analyze")
def analyze_question(data: QuestionInput):
    return run_pipeline(data.question)
